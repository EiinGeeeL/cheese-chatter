{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat History OakLang Graph\n",
    "Here we'll be interacting with a server that's exposing a chat bot with memmory history being persisted on the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "cheese = RemoteRunnable(\"http://localhost:8000/cheese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Graph Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the thread_id\n",
    "config = {\"configurable\": {\"thread_id\": \"001\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and invoke\n",
    "user_input = \"Ey, qué tal?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "for event in cheese.stream(message_input, config, stream_mode=\"updates\"):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and invoke\n",
    "user_input = \"Puedes resumirme los últimos 25 mensajes\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "for event in cheese.stream(message_input, config, stream_mode=\"updates\"):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "for event in cheese.stream(\n",
    "    # provide value\n",
    "    Command(resume={\"action\": \"continue\"}),\n",
    "    config,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Chat Cheese-Chatter Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Live chat via stream\n",
    "# define the thread_id\n",
    "config = {\"configurable\": {\"thread_id\": \"002\"}}\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Human: \")\n",
    "    message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Cheese-Chatter: Adiós!\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Human: {user_input}\") # to see the user input in the live chat\n",
    "    for event in cheese.stream(message_input, config): \n",
    "        for key, value in event.items():\n",
    "            if key == 'cheeseagent' and len(value[\"messages\"][-1].content) > 0:\n",
    "                print(\"Cheese-Chatter:\", value[\"messages\"][-1].content)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Follow the nodes via stream\n",
    "# define the thread_id\n",
    "config = {\"configurable\": {\"thread_id\": \"002\"}}\n",
    "\n",
    "# define the input\n",
    "user_input = \"Do you know what is an atom?\"\n",
    "message_input = {\"messages\": [{\"role\": \"human\", \"content\": user_input}]}\n",
    "for event in cheese.stream(message_input, config):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in event.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    url='http://localhost:8000/cheese/invoke',\n",
    "    json={\n",
    "        'input': {\n",
    "            \"messages\": [(\"human\", \"Hola, cómo estás\")],\n",
    "        },\n",
    "        'config': {\n",
    "            'configurable': {\n",
    "                \"thread_id\": \"004\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json()[\"output\"][\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test API\n",
    "n_messages=25\n",
    "\n",
    "url = f'http://***:8083/api/messages?size={n_messages}'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    raise ValueError(f\"Error: {n_messages} is not a valid argument\")\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Filter the id and timestand and reverse the messages\n",
    "parsed_data = [{k: v for k, v in item.items() if k != 'timestamp' and k != 'id'} for item in reversed(data)]\n",
    "\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST HUMAN REVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST CUSTOM GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cheese.entity.statehandler import StateCommander\n",
    "from typing_extensions import Literal, Any\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Command, interrupt\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from IPython.display import Image, display\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "@tool\n",
    "def weather_search(city: str):\n",
    "    \"\"\"Search for the weather\"\"\"\n",
    "    print(\"----\")\n",
    "    print(f\"Searching for: {city}\")\n",
    "    print(\"----\")\n",
    "    return \"Sunny!\"\n",
    "\n",
    "\n",
    "model = ChatOllama(model='llama3.1', temperature=0).bind_tools([weather_search])\n",
    "\n",
    "class State(MessagesState):\n",
    "    \"\"\"Simple state.\"\"\"\n",
    "\n",
    "\n",
    "def call_llm(state):\n",
    "    return {\"messages\": [model.invoke(state[\"messages\"])]}\n",
    "\n",
    "class CustomClass(StateCommander):\n",
    "    @staticmethod\n",
    "    def command(state:StateGraph(state_schema=Any)) -> Command[Literal['tool_node', 'call_llm']]: # type: ignore\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        tool_call = last_message.tool_calls[-1]\n",
    "\n",
    "        # this is the value we'll be providing via Command(resume=<human_review>)\n",
    "        human_review = interrupt(\n",
    "            {\n",
    "                \"question\": \"Is this correct?\",\n",
    "                # Surface tool calls for review\n",
    "                \"tool_call\": tool_call,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        review_action = human_review[\"action\"]\n",
    "        review_data = human_review.get(\"data\")\n",
    "\n",
    "        # if approved, call the tool\n",
    "        if review_action == \"continue\":\n",
    "            return Command(goto='tool_node')\n",
    "\n",
    "        # update the AI message AND call tools\n",
    "        elif review_action == \"update\":\n",
    "            updated_message = {\n",
    "                \"role\": \"ai\",\n",
    "                \"content\": last_message.content,\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": tool_call[\"id\"],\n",
    "                        \"name\": tool_call[\"name\"],\n",
    "                        # This the update provided by the human\n",
    "                        \"args\": review_data,\n",
    "                    }\n",
    "                ],\n",
    "                # This is important - this needs to be the same as the message you replacing!\n",
    "                # Otherwise, it will show up as a separate message\n",
    "                \"id\": last_message.id,\n",
    "            }\n",
    "            return Command(goto='tool_node', update={\"messages\": [updated_message]})\n",
    "\n",
    "        # provide feedback to LLM\n",
    "        elif review_action == \"feedback\":\n",
    "            # NOTE: we're adding feedback message as a ToolMessage\n",
    "            # to preserve the correct order in the message history\n",
    "            # (AI messages with tool calls need to be followed by tool call messages)\n",
    "            tool_message = {\n",
    "                \"role\": \"tool\",\n",
    "                # This is our natural language feedback\n",
    "                \"content\": review_data,\n",
    "                \"name\": tool_call[\"name\"],\n",
    "                \"tool_call_id\": tool_call[\"id\"],\n",
    "            }\n",
    "            return Command(goto='call_llm', update={\"messages\": [tool_message]})\n",
    "\n",
    "\n",
    "def route_after_llm(state) -> Literal['end', 'review']:\n",
    "    if len(state[\"messages\"][-1].tool_calls) == 0:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"review\"\n",
    "\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"call_llm\", call_llm)\n",
    "builder.add_node(\"tool_node\", ToolNode(tools=[weather_search]))\n",
    "builder.add_node(\"human_review_node\", CustomClass.command)\n",
    "builder.add_edge(START, \"call_llm\")\n",
    "builder.add_conditional_edges(\"call_llm\", route_after_llm, {\n",
    "                                  \"end\": END, # If `tools not needed`, then we call the tool node.\n",
    "                                  \"review\": \"human_review_node\", # Otherwise we finish.\n",
    "                                  })\n",
    "builder.add_edge(\"tool_node\", \"call_llm\")\n",
    "\n",
    "# Set up memory\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Add\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST GRAPH HUMAN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From LangServer\n",
    "from langgraph.types import Command\n",
    "from langserve import RemoteRunnable\n",
    "\n",
    "graph = RemoteRunnable(\"http://localhost:8000/cheese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From LangGraph\n",
    "from langgraph.types import Command\n",
    "from cheese.workflow_builder import WorkflowBuilder\n",
    "from cheese.config.config_graph import ConfigGraph\n",
    "\n",
    "## Workflow Configuration\n",
    "workflow_builder = WorkflowBuilder(config=ConfigGraph)\n",
    "graph = workflow_builder.compile() # compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": [{\"role\": \"human\", \"content\": \"cómo te encuentras?\"}]}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"updates\"):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "# initial_input = {\"messages\": [{\"role\": \"human\", \"content\": \"qué tiempo hace en sf?\"}]}\n",
    "initial_input = {\"messages\": [{\"role\": \"human\", \"content\": \"quieron un resumen 5000 mensajes\"}]}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"updates\"):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now continue executing from here\n",
    "for event in graph.stream(\n",
    "    # provide our natural language feedback!\n",
    "    Command(\n",
    "        resume={\n",
    "            \"action\": \"feedback\",\n",
    "            \"data\": \"Perdona, quise decir 5 mensajes\",\n",
    "        }\n",
    "    ),\n",
    "    thread,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in graph.stream(\n",
    "    # provide value\n",
    "    Command(resume={\"action\": \"continue\"}),\n",
    "    thread,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST NO STREAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": [{\"role\": \"human\", \"content\": \"cómo te encuentras?\"}]}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "graph.invoke(initial_input, thread, stream_mode=\"updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "initial_input = {\"messages\": [{\"role\": \"human\", \"content\": \"Quiero un resumen de 5000 mensajes.\"}]}\n",
    "\n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "graph.invoke(initial_input, thread, stream_mode=\"updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now continue executing from here\n",
    "graph.invoke(\n",
    "    # provide our natural language feedback!\n",
    "    Command(\n",
    "        resume={\n",
    "            \"action\": \"feedback\",\n",
    "            \"data\": \"Perdona, quise decir 5 mensajes\",\n",
    "        }\n",
    "    ),\n",
    "    thread,\n",
    "    stream_mode=\"updates\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\n",
    "    # provide value\n",
    "    Command(resume={\"action\": \"continue\"}),\n",
    "    thread,\n",
    "    stream_mode=\"updates\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
